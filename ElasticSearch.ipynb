{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import jieba\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.client import IndicesClient\n",
    "import mysql.connector as sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host':'localhost','port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_idx = IndicesClient(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_idx.delete(index='_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"main_idx\"}\n"
     ]
    }
   ],
   "source": [
    "a = os.popen('''\n",
    "curl -X PUT 'localhost:9200/main_idx' -d '\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"docs\": {\n",
    "      \"properties\": {\n",
    "        \"pure_text\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"ik_max_word\",\n",
    "          \"search_analyzer\": \"ik_max_word\"\n",
    "        },\n",
    "        \"processed_text\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"ik_max_word\",\n",
    "          \"search_analyzer\": \"ik_max_word\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'\n",
    "''').read()\n",
    "for line in a.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"main_idx\",\"_type\":\"docs\",\"_id\":\"1\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"created\":true}\n"
     ]
    }
   ],
   "source": [
    "# res1 = os.popen('''\n",
    "# curl -X PUT 'localhost:9200/main_idx/docs/1' -d '\n",
    "# {\n",
    "#     \"pure_text\": \"中华人民共和国国歌\",\n",
    "#     \"processed_text\": \"中华 人民 共和国 国歌\"\n",
    "# }'\n",
    "# ''').read()\n",
    "# for line in res1.split('\\n'):\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"took\":1,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":1,\"max_score\":0.25811607,\"hits\":[{\"_index\":\"main_idx\",\"_type\":\"docs\",\"_id\":\"1\",\"_score\":0.25811607,\"_source\":\n",
      "{\n",
      "    \"pure_text\": \"中华人民共和国国歌\",\n",
      "    \"processed_text\": \"中华 人民 共和国 国歌\"\n",
      "}}]}}\n"
     ]
    }
   ],
   "source": [
    "# res = os.popen('''\n",
    "# curl 'localhost:9200/main_idx/docs/_search'  -d '\n",
    "# {\n",
    "#   \"query\" : { \"match\" : { \"pure_text\" : \"中华人民共和国\" }}\n",
    "# }'\n",
    "# ''').read()\n",
    "# for line in res.split('\\n'):\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_idx.exists(index='main_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_question:  1970\n",
      "all_question:  1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.487 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Description</th>\n",
       "      <th>KeyWords</th>\n",
       "      <th>Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1420656.0</td>\n",
       "      <td>赫赛汀  配了之后结冰了， 能否继续使用。</td>\n",
       "      <td>使用 赫赛汀</td>\n",
       "      <td>赫赛汀  配  之后  结冰  继续  使用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1423956.0</td>\n",
       "      <td>赫赛汀粉末形状如何？患者保存的赫赛汀的稀释液出现结冰情况，是否能否使用</td>\n",
       "      <td>无 使用 患者 赫赛汀</td>\n",
       "      <td>赫赛汀  粉末  形状  保存  赫赛汀  稀释  稀释液  出现  结冰  冰情  情况 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1434184.0</td>\n",
       "      <td>请帮忙查一些赫赛汀与血栓栓塞方面的数据和文献</td>\n",
       "      <td>栓塞 查 赫赛汀 文献 血栓栓塞 无 血栓</td>\n",
       "      <td>帮忙  查  赫赛汀  血栓  血栓栓塞  栓塞  方面  数据  和文  文献</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1436011.0</td>\n",
       "      <td>拿到的赫赛汀呈现块状是否正常</td>\n",
       "      <td>正常 赫赛汀</td>\n",
       "      <td>拿到  赫赛汀  呈现  块状  是否  正常</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1438071.0</td>\n",
       "      <td>常规配置和保存的赫赛汀复溶后的溶液保存在2-8度的情况下，现出现浑浊情况，能否使用</td>\n",
       "      <td>赫赛汀 配置 现 下 溶液 使用</td>\n",
       "      <td>常规  配置  保存  赫赛汀  复  溶  溶液  保存  存在  2  8  度  情况...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case Number                                Description  \\\n",
       "0    1420656.0                      赫赛汀  配了之后结冰了， 能否继续使用。   \n",
       "1    1423956.0        赫赛汀粉末形状如何？患者保存的赫赛汀的稀释液出现结冰情况，是否能否使用   \n",
       "2    1434184.0                     请帮忙查一些赫赛汀与血栓栓塞方面的数据和文献   \n",
       "3    1436011.0                             拿到的赫赛汀呈现块状是否正常   \n",
       "4    1438071.0  常规配置和保存的赫赛汀复溶后的溶液保存在2-8度的情况下，现出现浑浊情况，能否使用   \n",
       "\n",
       "                KeyWords                                          Processed  \n",
       "0                 使用 赫赛汀                             赫赛汀  配  之后  结冰  继续  使用  \n",
       "1            无 使用 患者 赫赛汀  赫赛汀  粉末  形状  保存  赫赛汀  稀释  稀释液  出现  结冰  冰情  情况 ...  \n",
       "2  栓塞 查 赫赛汀 文献 血栓栓塞 无 血栓           帮忙  查  赫赛汀  血栓  血栓栓塞  栓塞  方面  数据  和文  文献  \n",
       "3                 正常 赫赛汀                            拿到  赫赛汀  呈现  块状  是否  正常  \n",
       "4       赫赛汀 配置 现 下 溶液 使用  常规  配置  保存  赫赛汀  复  溶  溶液  保存  存在  2  8  度  情况...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_question = pd.read_csv('./data/description.csv')\n",
    "Herceptin_question = pd.read_excel('./data/Herceptin.xlsx')\n",
    "# all_question = pd.read_excel('./data/Herceptin_funding_filtered.xlsx')\n",
    "# all_question = pd.read_excel('./data/Herceptin_patient_filtered.xlsx')\n",
    "Kadcyla_question = pd.read_excel('./data/Kadcyla.xlsx')\n",
    "Perjeta_1_question = pd.read_excel('./data/Perjeta_1.xlsx')\n",
    "Perjeta_2_question = pd.read_excel('./data/Perjeta_2.xlsx')\n",
    "Tecentriq_question = pd.read_excel('./data/Tecentriq.xlsx')\n",
    "Xeloda_question = pd.read_excel('./data/Xeloda.xlsx')\n",
    "No_product_question = pd.read_excel('./data/No_product_merged.xlsx')\n",
    "\n",
    "all_question = pd.concat([Herceptin_question, Kadcyla_question, Perjeta_1_question, Perjeta_2_question, Tecentriq_question, Xeloda_question, No_product_question])\n",
    "print('all_question: ', len(all_question))\n",
    "all_question.head(5)\n",
    "\n",
    "all_question.dropna(axis=0, inplace=True, subset=['Description'])\n",
    "all_question.isna().sum()\n",
    "\n",
    "print('all_question: ', len(all_question))\n",
    "\n",
    "jieba.load_userdict('./dict data/chinese_word_dict.txt')\n",
    "chinese_word_dict = pickle.load(open('./dict data/chinese_word_dict.pickle', 'rb'))\n",
    "stopwords = [line.strip() for line in codecs.open('./stopwords-zh.txt', 'r', encoding='utf8').readlines()]\n",
    "\n",
    "def keyword_extract(text, word_dict):\n",
    "#     text = list(jieba.cut(text, cut_all=False))\n",
    "    text = list(jieba.cut_for_search(text))\n",
    "    union_text = list(set(text).intersection(set(word_dict)))\n",
    "    return \" \".join(union_text)\n",
    "\n",
    "all_question['KeyWords'] = all_question.Description.apply(lambda x: keyword_extract(x, chinese_word_dict))\n",
    "\n",
    "# 去除所有“[]”内的文本\n",
    "def remove_special_string(text):\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', text)\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'（[^）]*\\）', '', text)\n",
    "    text = re.sub(r'\\<[^>]*\\>', '', text)\n",
    "#     text = re.sub(r'[0-9]*', '', text)\n",
    "#     text = re.sub('患者', '', text)\n",
    "#     text = re.sub('家属', '', text)\n",
    "#     text = re.sub('咨询', '', text)\n",
    "#     text = re.sub('询问', '', text)\n",
    "    return text\n",
    "def remove_stopwords(text, stopwords):\n",
    "    text_cutted = jieba.cut(text, cut_all=True)\n",
    "    text_removed = []\n",
    "    for word in text_cutted:\n",
    "        if word in stopwords or word.strip()==\"\":\n",
    "            continue\n",
    "        text_removed.append(word)\n",
    "    return \"  \".join(text_removed)\n",
    "\n",
    "all_question['Description'] = all_question.Description.apply(lambda x: remove_special_string(x))\n",
    "all_question['Processed'] = all_question.Description.apply(lambda x: remove_special_string(x))\n",
    "all_question['Processed'] = all_question.Processed.apply(lambda x: remove_stopwords(x, stopwords))\n",
    "all_question.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_query_text = []\n",
    "for idx, row in all_question.iterrows():\n",
    "    all_query_text.append(str(row['Description']) + \"\\n\\n\")\n",
    "    \n",
    "# all_query_text_f = open('all_query_text.txt', 'w')\n",
    "# all_query_text_f.writelines(all_query_text)\n",
    "# all_query_text_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1656\n",
      "{'pure_text': '赫赛汀  配了之后结冰了， 能否继续使用。', 'processed_text': '赫赛汀 使用'}\n"
     ]
    }
   ],
   "source": [
    "doc_list = []\n",
    "for idx,row in all_question.iterrows():\n",
    "    doc = {\"pure_text\": row['Description'], \"processed_text\": row['KeyWords']}\n",
    "    doc_list.append(doc)\n",
    "print(len(doc_list))\n",
    "print(doc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"main_idx\",\"_type\":\"docs\",\"_id\":\"AWu2fenK8gCSk-fY15qv\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"created\":true}\n"
     ]
    }
   ],
   "source": [
    "res1 = os.popen('''\n",
    "curl -X POST 'localhost:9200/main_idx/docs' -d '\n",
    "{\n",
    "    \"pure_text\": \"这是一条样例数据，包含乳腺癌激酶这个词。\"\n",
    "}'\n",
    "''').read()\n",
    "for line in res1.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1656/1656 [01:54<00:00, 14.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(doc_list):\n",
    "    temp = os.popen('''\n",
    "    curl -X POST 'localhost:9200/main_idx/docs' -d '\n",
    "    {}\n",
    "    '\n",
    "    '''.format(str(doc).replace(\"\\\"\", \"\\'\").replace(\"\\'\", \"\\\"\"))).read()\n",
    "#     for line in temp.split('\\n'):\n",
    "#         print(line)\n",
    "        \n",
    "#     print('''\n",
    "#     curl -X PUT 'localhost:9200/ik_test/docs/{}' -d '\n",
    "#     {}\n",
    "#     '\n",
    "#     '''.format(str(idx), str(doc_list[idx]).replace(\"\\\"\", \"\\'\").replace(\"\\'\", \"\\\"\")))\n",
    "    \n",
    "# curl -X PUT 'localhost:9200/ik_test/docs/1' -d '\n",
    "# {\n",
    "#     \"content\": \"中华人民共和国国歌\"\n",
    "# }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = os.popen('''\n",
    "curl 'localhost:9200/main_idx/docs/_search?pretty=true'  -d '\n",
    "{\n",
    "  \"query\" : { \"match\" : { \"processed_text\" : \"关于赫赛汀心脏毒性：1、临床 2、临床前\" }}\n",
    "}'\n",
    "''').read()\n",
    "for line in res.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(doc_list)):\n",
    "    es.index(index='text_search', id=idx, body=doc_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"乳腺癌激酶\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  这是一条样例数据，包含乳腺癌激酶这个词。\n"
     ]
    }
   ],
   "source": [
    "# processed_text  pure_text\n",
    "results = es.search(index='main_idx', body=\n",
    "{\n",
    "  \"query\" : { \"match\" : { \"pure_text\" : query }}\n",
    "}\n",
    "# {\n",
    "#   \"query\": {\n",
    "#     \"bool\": {\n",
    "#       \"must\":     { \"match\": { \"processed_text\": query }},\n",
    "#       \"should\":   { \"match\": { \"pure_text\": \"帕妥珠单抗\" }}\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    ",size=10)\n",
    "hits = results['hits']['hits']\n",
    "# results\n",
    "for idx, res in enumerate(hits):\n",
    "#     if \"帕妥珠单抗\" in res['_source']['pure_text']:\n",
    "    print(str(idx+1) + \":  \" + res['_source']['pure_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是\n",
      "一条\n",
      "样\n",
      "例\n",
      "数据\n",
      "包含\n",
      "乳腺癌激酶\n",
      "这个词\n"
     ]
    }
   ],
   "source": [
    "tokens_res = es_idx.analyze(index='main_idx', body={\n",
    "#     \"analyzer\": \"ik_max_word\",\n",
    "    \"analyzer\": \"ik_smart\",\n",
    "    \"text\" : \"这是一条样例数据，包含乳腺癌激酶这个词\"\n",
    "})\n",
    "for token in tokens_res['tokens']:\n",
    "    print(token['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
